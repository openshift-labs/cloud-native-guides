## Monitoring Application Health 

In this section we will learn how to monitor application health using Kubernetes health probes.

### Health Probes
When building microservices, monitoring becomes of extreme importance to make sure all services 
are running at all times, and when they don't there are automatic actions triggered to rectify 
the issues. 

OpenShift, using Kubernetes health probes, offers a solution for monitoring application 
health and try to automatically heal faulty containers through restarting them to fix issues such as
a deadlock in the application which can be resolved by restarting the container. Restarting a container 
in such a state can help to make the application more available despite bugs.

Furthermore, there are of course a category of issues that can't be resolved by restarting the container. 
In those scenarios, OpenShift would remove the faulty container from the built-in load-balancer and send traffic 
only to the healthy container remained.

There are two type of health probes available in Kubernetes: liveness probes and readiness probes. Liveness 
probes are to know when to restart a container and readiness probes to know when a Container is ready to start 
accepting traffic.

Health probes also provide crucial benefits when automating deployments with practises like rolling updates in 
order to remove downtime during deployments.. A readiness health probe would signal OpenShift when to switch 
traffic from the old version of the container to the new version so that the users don't get affected during 
deployments.

There are {{OPENSHIFT_DOCS_BASE}}/dev_guide/application_health.html#container-health-checks-using-probes[three ways to define a health probe] for a container:

* *HTTP Checks:* healthiness of the container is determined based on the response code of an HTTP 
endpoint. Anything between 200 and 399 is considered success. A HTTP check is ideal for applications 
that return HTTP status codes when completely initialized.

* *Container Execution Checks:* a specificed command is executed inside the container and the healthiness is 
determined based on the return value (0 is success). 

* *TCP Socket Checks:* a socket is opened on a specified port to the container and it's considered healthy 
only if the check can establish a connection. TCP socket check is ideal for applications that do not 
start listening until initialization is complete.
 
Let's add health probes to the microservices deployed so far.

### Review Health REST Endpoints

Spring Boot, WildFly Swarm and Vert.x all provide out-of-the-box support for creating RESTful endpoints that
provide details on the health of the application. These endpoints by default provide basic data about the 
service however they all provide a way to customize the health data and add more meaningful information (e.g. 
database connection health, backoffice system availability, etc).

Let's check out the health info that is available on each service. First, make sure you are on 
the `{{COOLSTORE_PROJECT}}` project:

[source,bash]
----
$ oc project {{COOLSTORE_PROJECT}}
----

List the routes url in this project to know how to access each service:

[source,bash]
----
$ oc get routes

NAME        HOST/PORT                                                  PATH      SERVICES    PORT       TERMINATION   
catalog     catalog-coolstore.roadshow.openshiftapps.com               catalog     8080                     None
gateway     gateway-coolstore.roadshow.openshiftapps.com               gateway     8080                     None
inventory   inventory-coolstore.roadshow.openshiftapps.com             inventory   8080                     None
web         web-coolstore.roadshow.openshiftapps.com                   web         8080                     None
----

http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#production-ready[Spring Boot Actuator] is a 
sub-project of Spring Boot which adds health and management HTTP endpoints to the application. Enabling Spring Boot 
Actuator is done via adding `org.springframework.boot:spring-boot-starter-actuator` to the Maven project 
dependencies which is already done for the Catalog services.

Verify that the health endpoint works for the Catalog service using `curl`:

CAUTION: The route urls in your project would be different from the ones in this lab guide! Use the ones from yor project.

[source,bash]
----
$ curl http://CATALOG-ROUTE-URL/health

{"status":"UP","diskSpace":{"status":"UP","total":3209691136,"free":2667175936,"threshold":10485760},"db":{"status":"UP","database":"H2","hello":1}}
----

https://wildfly-swarm.gitbooks.io/wildfly-swarm-users-guide/content/advanced/monitoring.html[WildFly Swarm health endpoints] function in a similar fashion and are enabled by adding `org.wildfly.swarm:monitor` 
to the Maven project dependencies. 
This is also already done for the Inventory service.

Verify that the health endpoint works for the Inventory service using `curl`:

CAUTION: The route urls in your project would be different from the ones in this lab guide! Use the ones from yor project.

[source,bash]
----
$ curl http://INVENTORY-ROUTE-URL/health

{"checks": [
{"id":"Inventory Health","result":"UP","data": {"date":"Sat Jul 29 04:37:13 UTC 2017"}}],
"outcome": "UP"
}
----

Expectedly, Eclipse Vert.x also provides a http://vertx.io/docs/vertx-health-check/java/[health check module] 
which is enabled by adding `io.vertx:vertx-health-check` as a dependency to the Maven project. 

Verify that the health endpoint works for the Inventory service using `curl`:

CAUTION: The route urls in your project would be different from the ones in this lab guide! Use the ones from yor project.

[source,bash]
----
$ curl http://API-GATEWAY-ROUTE-URL/health

{"status":"UP"}
----

Last but not least, although you can build more sophisticated health endpoints for the Web UI as well, you 
can use the root context ("/") of the Web UI in this lab to verify it's up and running.

### Monitoring Catalog Service Health

Health probes are defined on the deployment config for each pod and can be added using OpenShift Web 
Console or OpenShift CLI. You will try both in this lab.

Like mentioned, health probes are defined on a deployment config for each pod. Review the available 
deployment configs in the project. 

[source,bash]
----
$ oc get dc

NAME        REVISION   DESIRED   CURRENT   TRIGGERED BY
catalog     4          1         1         config,image(catalog:latest)
gateway     2          1         1         config,image(gateway:latest)
inventory   1          1         1         config,image(inventory:latest)
web         4          1         1         config,image(web:latest)
----

TIP: `dc` stands for deployment config

Add a liveness probe on the catalog deployment config using `oc set probe`:

[source,bash]
----
$ oc set probe dc/catalog --liveness --get-url=http://:8080/health
----

NOTE: OpenShift automates deployments using {{OPENSHIFT_DOCS_BASE}}/dev_guide/deployments/basic_deployment_operations.html#triggers[triggers] that react to changes to the container image or configuration. Therefore, as soon as you define the probe, OpenShift automatically redeploys the Catalog pod using the new configuration including the liveness probe. 

The `--get-url` defines the HTTP endpoint to use for check the liveness of the container. The `http://8080` 
syntax is a convenient way to define the endpoint without having to worry about the hostname for the running 
container. 

It is possible to customize to probes even further using for example `--initial-delay-seconds` to specify how long 
to wait after the container starts and before to begin checking the probes. Run `oc set probe --help` to get 
a list of all available options.

Add a readiness probe on the catalog deployment config using the same `/health` endpoint that you used for 
the liveness probe.

TIP: It's recommended to have separate endpoints for readiness and liveness to indicate to OpenShift when 
to restart the container and when to leave it alone and remove it from the load-balancer so that an administrator 
would  manually investigate the issue. 

[source,bash]
----
$ oc set probe dc/catalog --readiness --get-url=http://:8080/health
----

Viola! OpenShift automatically {{OPENSHIFT_DOCS_BASE}}/dev_guide/deployments/basic_deployment_operations.html#triggers[restarts] 
the Catalog pod and as soon as the health probes succeed, it is ready to receive traffic. 

### Monitoring Inventory Service Health

Adding liveness and readiness probes can be done at the same time if you want to define the same health endpoint 
and parameters for both liveness and readiness probes. 

Add liveness and readiness probes to the Inventory service:

[source,bash]
----
$ oc set probe dc/inventory --liveness --readiness --get-url=http://:8080/health
----

OpenShift automatically restarts the Inventory pod and as soon as the health probes succeed, it is ready to receive traffic. 

Using the `oc describe` command, you can get a detailed look into the deployment config and verify that the health probes are in fact 
configured as you wanted:

[source,bash]
----
$ oc describe dc/inventory

Name:		inventory
Namespace:	coolstore
...
  Containers:
   wildfly-swarm:
    Image:		172.30.200.222:5000/coolstore/inventory@sha256:afbf4e134573bfaa61690d346bb76866a6881977558bec4f0db74390ac2be950
    Ports:		8080/TCP, 9779/TCP, 8778/TCP
    Liveness:		http-get http://:8080/health delay=180s timeout=1s period=10s #success=1 #failure=3
    Readiness:		http-get http://:8080/health delay=10s timeout=1s period=10s #success=1 #failure=3
...
----

### Monitoring API Gateway Health

You are an expert in health probes by now! Add liveness and readiness probes to the API Gateway service:

[source,bash]
----
$ oc set probe dc/gateway --liveness --readiness --get-url=http://:8080/health
----

OpenShift automatically restarts the Inventory pod and as soon as the health probes succeed, it is 
ready to receive traffic. 

### Monitoring Web UI Health

Although you can add the liveness and health probes to the Web UI using a single CLI command, let's 
give the OpenShift Web Console a try this time.

Go the OpenShift Web Console in your browser and in the {{COOLSTORE_PROJECT}} project. Click on 
*Applications* &rarr; *Deployments* on the left-side bar. Click on `web` and then the *Configuration` 
tab. You will see the warning about health checks, with a link to
click in order to add them. Click *Add health checks* now. 

TIP: Instead of *Configuration* tab, you can directly click on *Actions* button on the top-right 
and then *Edit Health Checks*

You will want to click both *Add Readiness Probe* and *Add Liveness Probe* and
then fill them out as follows:

_Readiness Probe_

* Path: `/health/`
* Initial Delay: `15`
* Timeout: `1`

_Liveness Probe_

* Path: `/ws/healthz/`
* Initial Delay: `15`
* Timeout: `1`

Click *Save* and then click the *Overview* button in the left navigation. You
will notice that Web UI pod is getting restarted and it stays light blue
for a while. This is a sign that the pod(s) have not yet passed their readiness
checks and it turns blue when it's ready!

### Monitoring Metrics